{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сгенерировать последовательности, которые бы состояли из цифр (от 0 до 9) и задавались следующим образом (25/50/75/100):\n",
    "x - последовательность цифр  \n",
    "y1 = x1, y(i) = x(i) + x(1).  \n",
    "Если y(i) >= 10, то y(i) = y(i) - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [int(char) for char in '0123456789']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 128\n",
    "message_length = 5\n",
    "\n",
    "\n",
    "def dataset(num_examples):\n",
    "    dataset = []\n",
    "    \n",
    "    for ex in range(num_examples):     \n",
    "        x = [random.choice(vocab) for x in range(message_length)]\n",
    "        \n",
    "        y = [x[0]]\n",
    "        for xi in x[1:]:\n",
    "            if (xi + x[0]) >= 10:\n",
    "                y.append((xi + x[0])%10)\n",
    "            else:\n",
    "                y.append(xi + x[0])        \n",
    "        \n",
    "        dataset.append([torch.tensor(x), torch.tensor(y)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([3, 1, 5, 8, 0]), tensor([3, 4, 8, 1, 3])],\n",
       " [tensor([4, 2, 9, 5, 6]), tensor([4, 6, 3, 9, 0])],\n",
       " [tensor([4, 9, 2, 9, 0]), tensor([4, 3, 6, 3, 4])],\n",
       " [tensor([0, 7, 4, 0, 5]), tensor([0, 7, 4, 0, 5])],\n",
       " [tensor([4, 7, 5, 9, 6]), tensor([4, 1, 9, 3, 0])],\n",
       " [tensor([0, 6, 0, 2, 4]), tensor([0, 6, 0, 2, 4])],\n",
       " [tensor([3, 4, 1, 6, 0]), tensor([3, 7, 4, 9, 3])],\n",
       " [tensor([6, 2, 3, 8, 5]), tensor([6, 8, 9, 4, 1])],\n",
       " [tensor([3, 1, 0, 8, 5]), tensor([3, 4, 3, 1, 8])],\n",
       " [tensor([1, 9, 6, 5, 9]), tensor([1, 0, 7, 6, 0])],\n",
       " [tensor([4, 5, 4, 8, 8]), tensor([4, 9, 8, 2, 2])],\n",
       " [tensor([7, 2, 1, 3, 1]), tensor([7, 9, 8, 0, 8])],\n",
       " [tensor([9, 2, 9, 1, 0]), tensor([9, 1, 8, 0, 9])],\n",
       " [tensor([0, 8, 3, 9, 0]), tensor([0, 8, 3, 9, 0])],\n",
       " [tensor([5, 1, 2, 0, 2]), tensor([5, 6, 7, 5, 7])],\n",
       " [tensor([4, 2, 6, 2, 2]), tensor([4, 6, 0, 6, 6])],\n",
       " [tensor([1, 2, 2, 3, 2]), tensor([1, 3, 3, 4, 3])],\n",
       " [tensor([1, 0, 9, 4, 0]), tensor([1, 1, 0, 5, 1])],\n",
       " [tensor([6, 2, 1, 6, 4]), tensor([6, 8, 7, 2, 0])],\n",
       " [tensor([9, 8, 3, 6, 2]), tensor([9, 7, 2, 5, 1])],\n",
       " [tensor([8, 9, 4, 2, 4]), tensor([8, 7, 2, 0, 2])],\n",
       " [tensor([4, 8, 6, 5, 5]), tensor([4, 2, 0, 9, 9])],\n",
       " [tensor([8, 8, 1, 3, 6]), tensor([8, 6, 9, 1, 4])],\n",
       " [tensor([9, 5, 4, 5, 8]), tensor([9, 4, 3, 4, 7])],\n",
       " [tensor([6, 0, 9, 9, 9]), tensor([6, 6, 5, 5, 5])],\n",
       " [tensor([1, 1, 4, 5, 8]), tensor([1, 2, 5, 6, 9])],\n",
       " [tensor([0, 6, 0, 7, 3]), tensor([0, 6, 0, 7, 3])],\n",
       " [tensor([8, 3, 0, 6, 5]), tensor([8, 1, 8, 4, 3])],\n",
       " [tensor([8, 8, 9, 3, 9]), tensor([8, 6, 7, 1, 7])],\n",
       " [tensor([0, 3, 8, 9, 6]), tensor([0, 3, 8, 9, 6])],\n",
       " [tensor([3, 7, 0, 7, 2]), tensor([3, 0, 3, 0, 5])],\n",
       " [tensor([7, 0, 4, 9, 5]), tensor([7, 7, 1, 6, 2])],\n",
       " [tensor([4, 3, 1, 7, 3]), tensor([4, 7, 5, 1, 7])],\n",
       " [tensor([4, 8, 1, 8, 1]), tensor([4, 2, 5, 2, 5])],\n",
       " [tensor([6, 4, 8, 0, 5]), tensor([6, 0, 4, 6, 1])],\n",
       " [tensor([9, 2, 7, 6, 2]), tensor([9, 1, 6, 5, 1])],\n",
       " [tensor([6, 4, 1, 1, 2]), tensor([6, 0, 7, 7, 8])],\n",
       " [tensor([4, 2, 0, 5, 3]), tensor([4, 6, 4, 9, 7])],\n",
       " [tensor([8, 4, 9, 7, 6]), tensor([8, 2, 7, 5, 4])],\n",
       " [tensor([3, 1, 4, 2, 1]), tensor([3, 4, 7, 5, 4])],\n",
       " [tensor([0, 7, 6, 3, 5]), tensor([0, 7, 6, 3, 5])],\n",
       " [tensor([6, 7, 3, 4, 5]), tensor([6, 3, 9, 0, 1])],\n",
       " [tensor([7, 1, 3, 4, 0]), tensor([7, 8, 0, 1, 7])],\n",
       " [tensor([6, 2, 6, 8, 1]), tensor([6, 8, 2, 4, 7])],\n",
       " [tensor([3, 0, 0, 9, 4]), tensor([3, 3, 3, 2, 7])],\n",
       " [tensor([5, 8, 8, 7, 6]), tensor([5, 3, 3, 2, 1])],\n",
       " [tensor([6, 8, 4, 3, 8]), tensor([6, 4, 0, 9, 4])],\n",
       " [tensor([5, 4, 2, 6, 5]), tensor([5, 9, 7, 1, 0])],\n",
       " [tensor([0, 2, 9, 2, 8]), tensor([0, 2, 9, 2, 8])],\n",
       " [tensor([0, 5, 1, 5, 0]), tensor([0, 5, 1, 5, 0])],\n",
       " [tensor([1, 0, 1, 7, 9]), tensor([1, 1, 2, 8, 0])],\n",
       " [tensor([9, 3, 9, 8, 5]), tensor([9, 2, 8, 7, 4])],\n",
       " [tensor([5, 2, 8, 6, 8]), tensor([5, 7, 3, 1, 3])],\n",
       " [tensor([0, 7, 5, 7, 0]), tensor([0, 7, 5, 7, 0])],\n",
       " [tensor([0, 8, 6, 7, 3]), tensor([0, 8, 6, 7, 3])],\n",
       " [tensor([2, 0, 5, 6, 2]), tensor([2, 2, 7, 8, 4])],\n",
       " [tensor([0, 0, 0, 1, 4]), tensor([0, 0, 0, 1, 4])],\n",
       " [tensor([4, 6, 5, 2, 7]), tensor([4, 0, 9, 6, 1])],\n",
       " [tensor([9, 5, 6, 8, 9]), tensor([9, 4, 5, 7, 8])],\n",
       " [tensor([9, 8, 6, 1, 9]), tensor([9, 7, 5, 0, 8])],\n",
       " [tensor([5, 0, 9, 9, 1]), tensor([5, 5, 4, 4, 6])],\n",
       " [tensor([7, 5, 6, 1, 6]), tensor([7, 2, 3, 8, 3])],\n",
       " [tensor([6, 3, 0, 9, 6]), tensor([6, 9, 6, 5, 2])],\n",
       " [tensor([1, 9, 1, 2, 5]), tensor([1, 0, 2, 3, 6])],\n",
       " [tensor([5, 0, 2, 7, 4]), tensor([5, 5, 7, 2, 9])],\n",
       " [tensor([5, 2, 4, 8, 8]), tensor([5, 7, 9, 3, 3])],\n",
       " [tensor([7, 6, 8, 0, 7]), tensor([7, 3, 5, 7, 4])],\n",
       " [tensor([5, 5, 6, 7, 6]), tensor([5, 0, 1, 2, 1])],\n",
       " [tensor([6, 5, 7, 4, 0]), tensor([6, 1, 3, 0, 6])],\n",
       " [tensor([9, 4, 2, 1, 2]), tensor([9, 3, 1, 0, 1])],\n",
       " [tensor([3, 4, 2, 4, 8]), tensor([3, 7, 5, 7, 1])],\n",
       " [tensor([4, 6, 0, 5, 1]), tensor([4, 0, 4, 9, 5])],\n",
       " [tensor([0, 9, 3, 4, 2]), tensor([0, 9, 3, 4, 2])],\n",
       " [tensor([9, 7, 9, 5, 8]), tensor([9, 6, 8, 4, 7])],\n",
       " [tensor([5, 1, 6, 5, 5]), tensor([5, 6, 1, 0, 0])],\n",
       " [tensor([9, 3, 4, 6, 0]), tensor([9, 2, 3, 5, 9])],\n",
       " [tensor([8, 1, 3, 4, 3]), tensor([8, 9, 1, 2, 1])],\n",
       " [tensor([6, 3, 2, 4, 8]), tensor([6, 9, 8, 0, 4])],\n",
       " [tensor([2, 6, 7, 4, 0]), tensor([2, 8, 9, 6, 2])],\n",
       " [tensor([7, 4, 6, 2, 2]), tensor([7, 1, 3, 9, 9])],\n",
       " [tensor([7, 8, 8, 8, 2]), tensor([7, 5, 5, 5, 9])],\n",
       " [tensor([7, 1, 4, 2, 8]), tensor([7, 8, 1, 9, 5])],\n",
       " [tensor([4, 6, 2, 0, 5]), tensor([4, 0, 6, 4, 9])],\n",
       " [tensor([6, 5, 7, 3, 0]), tensor([6, 1, 3, 9, 6])],\n",
       " [tensor([4, 4, 6, 4, 8]), tensor([4, 8, 0, 8, 2])],\n",
       " [tensor([6, 1, 1, 8, 9]), tensor([6, 7, 7, 4, 5])],\n",
       " [tensor([8, 7, 3, 0, 3]), tensor([8, 5, 1, 8, 1])],\n",
       " [tensor([3, 6, 0, 2, 8]), tensor([3, 9, 3, 5, 1])],\n",
       " [tensor([7, 7, 5, 2, 6]), tensor([7, 4, 2, 9, 3])],\n",
       " [tensor([0, 5, 8, 9, 0]), tensor([0, 5, 8, 9, 0])],\n",
       " [tensor([7, 9, 3, 2, 4]), tensor([7, 6, 0, 9, 1])],\n",
       " [tensor([2, 4, 2, 8, 8]), tensor([2, 6, 4, 0, 0])],\n",
       " [tensor([3, 3, 0, 6, 4]), tensor([3, 6, 3, 9, 7])],\n",
       " [tensor([4, 9, 8, 8, 6]), tensor([4, 3, 2, 2, 0])],\n",
       " [tensor([5, 3, 7, 3, 4]), tensor([5, 8, 2, 8, 9])],\n",
       " [tensor([4, 6, 9, 6, 2]), tensor([4, 0, 3, 0, 6])],\n",
       " [tensor([0, 8, 9, 0, 2]), tensor([0, 8, 9, 0, 2])],\n",
       " [tensor([3, 0, 1, 0, 7]), tensor([3, 3, 4, 3, 0])],\n",
       " [tensor([8, 8, 5, 1, 4]), tensor([8, 6, 3, 9, 2])],\n",
       " [tensor([5, 2, 8, 9, 6]), tensor([5, 7, 3, 4, 1])],\n",
       " [tensor([9, 7, 9, 5, 3]), tensor([9, 6, 8, 4, 2])],\n",
       " [tensor([5, 3, 5, 9, 6]), tensor([5, 8, 0, 4, 1])],\n",
       " [tensor([6, 0, 9, 4, 3]), tensor([6, 6, 5, 0, 9])],\n",
       " [tensor([0, 8, 1, 9, 0]), tensor([0, 8, 1, 9, 0])],\n",
       " [tensor([7, 0, 9, 5, 7]), tensor([7, 7, 6, 2, 4])],\n",
       " [tensor([5, 0, 4, 7, 8]), tensor([5, 5, 9, 2, 3])],\n",
       " [tensor([3, 8, 7, 2, 4]), tensor([3, 1, 0, 5, 7])],\n",
       " [tensor([9, 0, 1, 5, 4]), tensor([9, 9, 0, 4, 3])],\n",
       " [tensor([6, 8, 4, 5, 8]), tensor([6, 4, 0, 1, 4])],\n",
       " [tensor([8, 8, 3, 5, 5]), tensor([8, 6, 1, 3, 3])],\n",
       " [tensor([9, 3, 4, 0, 2]), tensor([9, 2, 3, 9, 1])],\n",
       " [tensor([3, 8, 4, 3, 2]), tensor([3, 1, 7, 6, 5])],\n",
       " [tensor([5, 5, 7, 5, 2]), tensor([5, 0, 2, 0, 7])],\n",
       " [tensor([0, 8, 4, 2, 0]), tensor([0, 8, 4, 2, 0])],\n",
       " [tensor([7, 5, 2, 6, 9]), tensor([7, 2, 9, 3, 6])],\n",
       " [tensor([1, 4, 2, 0, 9]), tensor([1, 5, 3, 1, 0])],\n",
       " [tensor([0, 8, 1, 6, 7]), tensor([0, 8, 1, 6, 7])],\n",
       " [tensor([3, 3, 8, 8, 2]), tensor([3, 6, 1, 1, 5])],\n",
       " [tensor([5, 0, 3, 3, 3]), tensor([5, 5, 8, 8, 8])],\n",
       " [tensor([1, 6, 5, 8, 6]), tensor([1, 7, 6, 9, 7])],\n",
       " [tensor([1, 8, 0, 0, 7]), tensor([1, 9, 1, 1, 8])],\n",
       " [tensor([4, 7, 7, 2, 8]), tensor([4, 1, 1, 6, 2])],\n",
       " [tensor([0, 6, 1, 9, 9]), tensor([0, 6, 1, 9, 9])],\n",
       " [tensor([7, 6, 7, 1, 3]), tensor([7, 3, 4, 8, 0])],\n",
       " [tensor([4, 3, 3, 4, 4]), tensor([4, 7, 7, 8, 8])],\n",
       " [tensor([0, 2, 3, 1, 8]), tensor([0, 2, 3, 1, 8])],\n",
       " [tensor([5, 6, 3, 0, 2]), tensor([5, 1, 8, 5, 7])],\n",
       " [tensor([9, 9, 9, 0, 8]), tensor([9, 8, 8, 9, 7])]]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset(num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача: научить модель предсказывать y(i) по x(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        ## Здесь создать слои\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embed(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.238, Train loss: 2.398\n",
      "Epoch 1. Time: 0.222, Train loss: 2.348\n",
      "Epoch 2. Time: 0.224, Train loss: 2.352\n",
      "Epoch 3. Time: 0.229, Train loss: 2.335\n",
      "Epoch 4. Time: 0.225, Train loss: 2.329\n",
      "Epoch 5. Time: 0.243, Train loss: 2.340\n",
      "Epoch 6. Time: 0.227, Train loss: 2.356\n",
      "Epoch 7. Time: 0.219, Train loss: 2.352\n",
      "Epoch 8. Time: 0.230, Train loss: 2.337\n",
      "Epoch 9. Time: 0.233, Train loss: 2.324\n",
      "Epoch 10. Time: 0.235, Train loss: 2.342\n",
      "Epoch 11. Time: 0.229, Train loss: 2.321\n",
      "Epoch 12. Time: 0.222, Train loss: 2.334\n",
      "Epoch 13. Time: 0.220, Train loss: 2.334\n",
      "Epoch 14. Time: 0.223, Train loss: 2.333\n",
      "Epoch 15. Time: 0.218, Train loss: 2.334\n",
      "Epoch 16. Time: 0.220, Train loss: 2.343\n",
      "Epoch 17. Time: 0.221, Train loss: 2.338\n",
      "Epoch 18. Time: 0.217, Train loss: 2.338\n",
      "Epoch 19. Time: 0.226, Train loss: 2.341\n",
      "Epoch 20. Time: 0.221, Train loss: 2.331\n",
      "Epoch 21. Time: 0.219, Train loss: 2.335\n",
      "Epoch 22. Time: 0.219, Train loss: 2.336\n",
      "Epoch 23. Time: 0.220, Train loss: 2.337\n",
      "Epoch 24. Time: 0.220, Train loss: 2.329\n",
      "Epoch 25. Time: 0.227, Train loss: 2.330\n",
      "Epoch 26. Time: 0.222, Train loss: 2.331\n",
      "Epoch 27. Time: 0.221, Train loss: 2.338\n",
      "Epoch 28. Time: 0.221, Train loss: 2.335\n",
      "Epoch 29. Time: 0.221, Train loss: 2.338\n",
      "Epoch 30. Time: 0.220, Train loss: 2.332\n",
      "Epoch 31. Time: 0.223, Train loss: 2.337\n",
      "Epoch 32. Time: 0.224, Train loss: 2.332\n",
      "Epoch 33. Time: 0.221, Train loss: 2.333\n",
      "Epoch 34. Time: 0.226, Train loss: 2.337\n",
      "Epoch 35. Time: 0.222, Train loss: 2.337\n",
      "Epoch 36. Time: 0.221, Train loss: 2.339\n",
      "Epoch 37. Time: 0.217, Train loss: 2.332\n",
      "Epoch 38. Time: 0.221, Train loss: 2.331\n",
      "Epoch 39. Time: 0.219, Train loss: 2.329\n",
      "Epoch 40. Time: 0.221, Train loss: 2.328\n",
      "Epoch 41. Time: 0.218, Train loss: 2.334\n",
      "Epoch 42. Time: 0.221, Train loss: 2.344\n",
      "Epoch 43. Time: 0.218, Train loss: 2.335\n",
      "Epoch 44. Time: 0.224, Train loss: 2.334\n",
      "Epoch 45. Time: 0.221, Train loss: 2.337\n",
      "Epoch 46. Time: 0.221, Train loss: 2.338\n",
      "Epoch 47. Time: 0.226, Train loss: 2.335\n",
      "Epoch 48. Time: 0.218, Train loss: 2.337\n",
      "Epoch 49. Time: 0.223, Train loss: 2.334\n",
      "Epoch 50. Time: 0.219, Train loss: 2.334\n",
      "Epoch 51. Time: 0.220, Train loss: 2.340\n",
      "Epoch 52. Time: 0.220, Train loss: 2.334\n",
      "Epoch 53. Time: 0.226, Train loss: 2.332\n",
      "Epoch 54. Time: 0.222, Train loss: 2.341\n",
      "Epoch 55. Time: 0.223, Train loss: 2.339\n",
      "Epoch 56. Time: 0.225, Train loss: 2.329\n",
      "Epoch 57. Time: 0.221, Train loss: 2.332\n",
      "Epoch 58. Time: 0.221, Train loss: 2.333\n",
      "Epoch 59. Time: 0.222, Train loss: 2.335\n",
      "Epoch 60. Time: 0.223, Train loss: 2.345\n",
      "Epoch 61. Time: 0.223, Train loss: 2.327\n",
      "Epoch 62. Time: 0.224, Train loss: 2.340\n",
      "Epoch 63. Time: 0.221, Train loss: 2.343\n",
      "Epoch 64. Time: 0.221, Train loss: 2.331\n",
      "Epoch 65. Time: 0.220, Train loss: 2.348\n",
      "Epoch 66. Time: 0.223, Train loss: 2.329\n",
      "Epoch 67. Time: 0.223, Train loss: 2.338\n",
      "Epoch 68. Time: 0.221, Train loss: 2.336\n",
      "Epoch 69. Time: 0.221, Train loss: 2.335\n",
      "Epoch 70. Time: 0.221, Train loss: 2.336\n",
      "Epoch 71. Time: 0.223, Train loss: 2.333\n",
      "Epoch 72. Time: 0.222, Train loss: 2.330\n",
      "Epoch 73. Time: 0.221, Train loss: 2.342\n",
      "Epoch 74. Time: 0.220, Train loss: 2.334\n",
      "Epoch 75. Time: 0.224, Train loss: 2.334\n",
      "Epoch 76. Time: 0.220, Train loss: 2.333\n",
      "Epoch 77. Time: 0.222, Train loss: 2.337\n",
      "Epoch 78. Time: 0.221, Train loss: 2.344\n",
      "Epoch 79. Time: 0.223, Train loss: 2.331\n",
      "Epoch 80. Time: 0.224, Train loss: 2.330\n",
      "Epoch 81. Time: 0.224, Train loss: 2.331\n",
      "Epoch 82. Time: 0.223, Train loss: 2.334\n",
      "Epoch 83. Time: 0.223, Train loss: 2.329\n",
      "Epoch 84. Time: 0.225, Train loss: 2.334\n",
      "Epoch 85. Time: 0.223, Train loss: 2.329\n",
      "Epoch 86. Time: 0.225, Train loss: 2.332\n",
      "Epoch 87. Time: 0.222, Train loss: 2.333\n",
      "Epoch 88. Time: 0.223, Train loss: 2.343\n",
      "Epoch 89. Time: 0.222, Train loss: 2.337\n",
      "Epoch 90. Time: 0.224, Train loss: 2.332\n",
      "Epoch 91. Time: 0.219, Train loss: 2.336\n",
      "Epoch 92. Time: 0.226, Train loss: 2.336\n",
      "Epoch 93. Time: 0.225, Train loss: 2.337\n",
      "Epoch 94. Time: 0.240, Train loss: 2.337\n",
      "Epoch 95. Time: 0.221, Train loss: 2.332\n",
      "Epoch 96. Time: 0.223, Train loss: 2.338\n",
      "Epoch 97. Time: 0.222, Train loss: 2.336\n",
      "Epoch 98. Time: 0.223, Train loss: 2.336\n",
      "Epoch 99. Time: 0.220, Train loss: 2.333\n"
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for x, y in dataset(num_examples):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(x.unsqueeze(1))\n",
    "        answers = answers.view(-1, vocab_size)\n",
    "        loss = criterion(answers, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.76%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for x, y in dataset(num_examples):\n",
    "            answers = model.forward(x.unsqueeze(1))\n",
    "            predictions = torch.nn.functional.softmax(answers, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, y).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        ## Здесь создать слои\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embed(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.263, Train loss: 2.322\n",
      "Epoch 1. Time: 0.260, Train loss: 2.307\n",
      "Epoch 2. Time: 0.257, Train loss: 2.311\n",
      "Epoch 3. Time: 0.256, Train loss: 2.309\n",
      "Epoch 4. Time: 0.258, Train loss: 2.306\n",
      "Epoch 5. Time: 0.257, Train loss: 2.306\n",
      "Epoch 6. Time: 0.260, Train loss: 2.307\n",
      "Epoch 7. Time: 0.258, Train loss: 2.307\n",
      "Epoch 8. Time: 0.256, Train loss: 2.307\n",
      "Epoch 9. Time: 0.259, Train loss: 2.306\n",
      "Epoch 10. Time: 0.256, Train loss: 2.306\n",
      "Epoch 11. Time: 0.256, Train loss: 2.306\n",
      "Epoch 12. Time: 0.255, Train loss: 2.306\n",
      "Epoch 13. Time: 0.259, Train loss: 2.307\n",
      "Epoch 14. Time: 0.256, Train loss: 2.306\n",
      "Epoch 15. Time: 0.258, Train loss: 2.306\n",
      "Epoch 16. Time: 0.257, Train loss: 2.306\n",
      "Epoch 17. Time: 0.260, Train loss: 2.306\n",
      "Epoch 18. Time: 0.254, Train loss: 2.308\n",
      "Epoch 19. Time: 0.257, Train loss: 2.307\n",
      "Epoch 20. Time: 0.256, Train loss: 2.312\n",
      "Epoch 21. Time: 0.259, Train loss: 2.343\n",
      "Epoch 22. Time: 0.255, Train loss: 2.306\n",
      "Epoch 23. Time: 0.257, Train loss: 2.306\n",
      "Epoch 24. Time: 0.258, Train loss: 2.306\n",
      "Epoch 25. Time: 0.259, Train loss: 2.305\n",
      "Epoch 26. Time: 0.256, Train loss: 2.306\n",
      "Epoch 27. Time: 0.258, Train loss: 2.307\n",
      "Epoch 28. Time: 0.257, Train loss: 2.307\n",
      "Epoch 29. Time: 0.257, Train loss: 2.306\n",
      "Epoch 30. Time: 0.255, Train loss: 2.306\n",
      "Epoch 31. Time: 0.258, Train loss: 2.308\n",
      "Epoch 32. Time: 0.262, Train loss: 2.306\n",
      "Epoch 33. Time: 0.259, Train loss: 2.307\n",
      "Epoch 34. Time: 0.255, Train loss: 2.308\n",
      "Epoch 35. Time: 0.258, Train loss: 2.306\n",
      "Epoch 36. Time: 0.255, Train loss: 2.306\n",
      "Epoch 37. Time: 0.258, Train loss: 2.306\n",
      "Epoch 38. Time: 0.258, Train loss: 2.305\n",
      "Epoch 39. Time: 0.258, Train loss: 2.306\n",
      "Epoch 40. Time: 0.258, Train loss: 2.305\n",
      "Epoch 41. Time: 0.256, Train loss: 2.306\n",
      "Epoch 42. Time: 0.275, Train loss: 2.305\n",
      "Epoch 43. Time: 0.257, Train loss: 2.306\n",
      "Epoch 44. Time: 0.257, Train loss: 2.305\n",
      "Epoch 45. Time: 0.257, Train loss: 2.305\n",
      "Epoch 46. Time: 0.259, Train loss: 2.307\n",
      "Epoch 47. Time: 0.258, Train loss: 2.306\n",
      "Epoch 48. Time: 0.257, Train loss: 2.306\n",
      "Epoch 49. Time: 0.256, Train loss: 2.306\n",
      "Epoch 50. Time: 0.255, Train loss: 2.306\n",
      "Epoch 51. Time: 0.257, Train loss: 2.306\n",
      "Epoch 52. Time: 0.258, Train loss: 2.306\n",
      "Epoch 53. Time: 0.257, Train loss: 2.306\n",
      "Epoch 54. Time: 0.257, Train loss: 2.307\n",
      "Epoch 55. Time: 0.258, Train loss: 2.307\n",
      "Epoch 56. Time: 0.260, Train loss: 2.306\n",
      "Epoch 57. Time: 0.259, Train loss: 2.306\n",
      "Epoch 58. Time: 0.255, Train loss: 2.306\n",
      "Epoch 59. Time: 0.259, Train loss: 2.306\n",
      "Epoch 60. Time: 0.258, Train loss: 2.306\n",
      "Epoch 61. Time: 0.256, Train loss: 2.306\n",
      "Epoch 62. Time: 0.257, Train loss: 2.307\n",
      "Epoch 63. Time: 0.258, Train loss: 2.306\n",
      "Epoch 64. Time: 0.258, Train loss: 2.306\n",
      "Epoch 65. Time: 0.257, Train loss: 2.306\n",
      "Epoch 66. Time: 0.257, Train loss: 2.305\n",
      "Epoch 67. Time: 0.257, Train loss: 2.305\n",
      "Epoch 68. Time: 0.256, Train loss: 2.306\n",
      "Epoch 69. Time: 0.257, Train loss: 2.305\n",
      "Epoch 70. Time: 0.259, Train loss: 2.306\n",
      "Epoch 71. Time: 0.266, Train loss: 2.306\n",
      "Epoch 72. Time: 0.258, Train loss: 2.308\n",
      "Epoch 73. Time: 0.258, Train loss: 2.307\n",
      "Epoch 74. Time: 0.257, Train loss: 2.306\n",
      "Epoch 75. Time: 0.259, Train loss: 2.307\n",
      "Epoch 76. Time: 0.256, Train loss: 2.305\n",
      "Epoch 77. Time: 0.257, Train loss: 2.306\n",
      "Epoch 78. Time: 0.259, Train loss: 2.306\n",
      "Epoch 79. Time: 0.260, Train loss: 2.306\n",
      "Epoch 80. Time: 0.255, Train loss: 2.306\n",
      "Epoch 81. Time: 0.258, Train loss: 2.307\n",
      "Epoch 82. Time: 0.257, Train loss: 2.306\n",
      "Epoch 83. Time: 0.259, Train loss: 2.307\n",
      "Epoch 84. Time: 0.257, Train loss: 2.306\n",
      "Epoch 85. Time: 0.258, Train loss: 2.305\n",
      "Epoch 86. Time: 0.257, Train loss: 2.307\n",
      "Epoch 87. Time: 0.260, Train loss: 2.306\n",
      "Epoch 88. Time: 0.256, Train loss: 2.306\n",
      "Epoch 89. Time: 0.258, Train loss: 2.306\n",
      "Epoch 90. Time: 0.258, Train loss: 2.305\n",
      "Epoch 91. Time: 0.258, Train loss: 2.307\n",
      "Epoch 92. Time: 0.258, Train loss: 2.306\n",
      "Epoch 93. Time: 0.258, Train loss: 2.307\n",
      "Epoch 94. Time: 0.261, Train loss: 2.307\n",
      "Epoch 95. Time: 0.260, Train loss: 2.307\n",
      "Epoch 96. Time: 0.259, Train loss: 2.306\n",
      "Epoch 97. Time: 0.260, Train loss: 2.306\n",
      "Epoch 98. Time: 0.258, Train loss: 2.307\n",
      "Epoch 99. Time: 0.257, Train loss: 2.306\n"
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for x, y in dataset(num_examples):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(x.unsqueeze(1))\n",
    "        answers = answers.view(-1, vocab_size)\n",
    "        loss = criterion(answers, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.59%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for x, y in dataset(num_examples):\n",
    "            answers = model.forward(x.unsqueeze(1))\n",
    "            predictions = torch.nn.functional.softmax(answers, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, y).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "softmax = torch.nn.functional.softmax\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) +\n",
    "                             list(lstm.parameters()) +\n",
    "                             list(linear.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_hidden():\n",
    "    return (torch.zeros(1, 1, hidden_dim),\n",
    "            torch.zeros(1, 1, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 2.3290\n",
      "Epoch: 1\n",
      "Loss: 2.2056\n",
      "Epoch: 2\n",
      "Loss: 2.3470\n",
      "Epoch: 3\n",
      "Loss: 2.2497\n",
      "Epoch: 4\n",
      "Loss: 2.3034\n",
      "Epoch: 5\n",
      "Loss: 2.3193\n",
      "Epoch: 6\n",
      "Loss: 2.3088\n",
      "Epoch: 7\n",
      "Loss: 2.3051\n",
      "Epoch: 8\n",
      "Loss: 2.3012\n",
      "Epoch: 9\n",
      "Loss: 2.3031\n",
      "Epoch: 10\n",
      "Loss: 2.2789\n",
      "Epoch: 11\n",
      "Loss: 2.3381\n",
      "Epoch: 12\n",
      "Loss: 2.3098\n",
      "Epoch: 13\n",
      "Loss: 2.3160\n",
      "Epoch: 14\n",
      "Loss: 2.3141\n",
      "Epoch: 15\n",
      "Loss: 2.3315\n",
      "Epoch: 16\n",
      "Loss: 2.3067\n",
      "Epoch: 17\n",
      "Loss: 2.2952\n",
      "Epoch: 18\n",
      "Loss: 2.3108\n",
      "Epoch: 19\n",
      "Loss: 2.3077\n",
      "Epoch: 20\n",
      "Loss: 2.3212\n",
      "Epoch: 21\n",
      "Loss: 2.2839\n",
      "Epoch: 22\n",
      "Loss: 2.3087\n",
      "Epoch: 23\n",
      "Loss: 2.3056\n",
      "Epoch: 24\n",
      "Loss: 2.3212\n",
      "Epoch: 25\n",
      "Loss: 2.3234\n",
      "Epoch: 26\n",
      "Loss: 2.3015\n",
      "Epoch: 27\n",
      "Loss: 2.3128\n",
      "Epoch: 28\n",
      "Loss: 2.3027\n",
      "Epoch: 29\n",
      "Loss: 2.2990\n",
      "Epoch: 30\n",
      "Loss: 2.3372\n",
      "Epoch: 31\n",
      "Loss: 2.2829\n",
      "Epoch: 32\n",
      "Loss: 2.3022\n",
      "Epoch: 33\n",
      "Loss: 2.3302\n",
      "Epoch: 34\n",
      "Loss: 2.3651\n",
      "Epoch: 35\n",
      "Loss: 2.3033\n",
      "Epoch: 36\n",
      "Loss: 2.3054\n",
      "Epoch: 37\n",
      "Loss: 2.3106\n",
      "Epoch: 38\n",
      "Loss: 2.3374\n",
      "Epoch: 39\n",
      "Loss: 2.3097\n",
      "Epoch: 40\n",
      "Loss: 2.3050\n",
      "Epoch: 41\n",
      "Loss: 2.3017\n",
      "Epoch: 42\n",
      "Loss: 2.3677\n",
      "Epoch: 43\n",
      "Loss: 2.2918\n",
      "Epoch: 44\n",
      "Loss: 2.3164\n",
      "Epoch: 45\n",
      "Loss: 2.3120\n",
      "Epoch: 46\n",
      "Loss: 2.3204\n",
      "Epoch: 47\n",
      "Loss: 2.3421\n",
      "Epoch: 48\n",
      "Loss: 2.3045\n",
      "Epoch: 49\n",
      "Loss: 2.3207\n",
      "Epoch: 50\n",
      "Loss: 2.3412\n",
      "Epoch: 51\n",
      "Loss: 2.3316\n",
      "Epoch: 52\n",
      "Loss: 2.3319\n",
      "Epoch: 53\n",
      "Loss: 2.3355\n",
      "Epoch: 54\n",
      "Loss: 2.3299\n",
      "Epoch: 55\n",
      "Loss: 2.3040\n",
      "Epoch: 56\n",
      "Loss: 2.3228\n",
      "Epoch: 57\n",
      "Loss: 2.3146\n",
      "Epoch: 58\n",
      "Loss: 2.3216\n",
      "Epoch: 59\n",
      "Loss: 2.3243\n",
      "Epoch: 60\n",
      "Loss: 2.3258\n",
      "Epoch: 61\n",
      "Loss: 2.3266\n",
      "Epoch: 62\n",
      "Loss: 2.3363\n",
      "Epoch: 63\n",
      "Loss: 2.3097\n",
      "Epoch: 64\n",
      "Loss: 2.3674\n",
      "Epoch: 65\n",
      "Loss: 2.3216\n",
      "Epoch: 66\n",
      "Loss: 2.3454\n",
      "Epoch: 67\n",
      "Loss: 2.3077\n",
      "Epoch: 68\n",
      "Loss: 2.3083\n",
      "Epoch: 69\n",
      "Loss: 2.3308\n",
      "Epoch: 70\n",
      "Loss: 2.3576\n",
      "Epoch: 71\n",
      "Loss: 2.3669\n",
      "Epoch: 72\n",
      "Loss: 2.3052\n",
      "Epoch: 73\n",
      "Loss: 2.2686\n",
      "Epoch: 74\n",
      "Loss: 2.3137\n",
      "Epoch: 75\n",
      "Loss: 2.3364\n",
      "Epoch: 76\n",
      "Loss: 2.3421\n",
      "Epoch: 77\n",
      "Loss: 2.3277\n",
      "Epoch: 78\n",
      "Loss: 2.3273\n",
      "Epoch: 79\n",
      "Loss: 2.3319\n",
      "Epoch: 80\n",
      "Loss: 2.3491\n",
      "Epoch: 81\n",
      "Loss: 2.3555\n",
      "Epoch: 82\n",
      "Loss: 2.3199\n",
      "Epoch: 83\n",
      "Loss: 2.2927\n",
      "Epoch: 84\n",
      "Loss: 2.3676\n",
      "Epoch: 85\n",
      "Loss: 2.3425\n",
      "Epoch: 86\n",
      "Loss: 2.3710\n",
      "Epoch: 87\n",
      "Loss: 2.3226\n",
      "Epoch: 88\n",
      "Loss: 2.3227\n",
      "Epoch: 89\n",
      "Loss: 2.3229\n",
      "Epoch: 90\n",
      "Loss: 2.3216\n",
      "Epoch: 91\n",
      "Loss: 2.3318\n",
      "Epoch: 92\n",
      "Loss: 2.3112\n",
      "Epoch: 93\n",
      "Loss: 2.3617\n",
      "Epoch: 94\n",
      "Loss: 2.3494\n",
      "Epoch: 95\n",
      "Loss: 2.3168\n",
      "Epoch: 96\n",
      "Loss: 2.3257\n",
      "Epoch: 97\n",
      "Loss: 2.3286\n",
      "Epoch: 98\n",
      "Loss: 2.3041\n",
      "Epoch: 99\n",
      "Loss: 2.3249\n"
     ]
    }
   ],
   "source": [
    "accuracies, max_accuracy = [], 0\n",
    "for ep in range(num_epochs):\n",
    "    print('Epoch: {}'.format(ep))\n",
    "    for x, y in dataset(num_examples):\n",
    "        lstm_in = embed(x)\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "        scores = linear(lstm_out)\n",
    "        scores = scores.transpose(1, 2)\n",
    "        y = y.unsqueeze(1)\n",
    "        loss = loss_fn(scores, y) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:6.4f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 9.62%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for x, y in dataset(num_examples):\n",
    "            lstm_in = embed(x)\n",
    "            lstm_in = lstm_in.unsqueeze(1)\n",
    "            lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "            scores = linear(lstm_out)\n",
    "            predictions = softmax(scores, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, y).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len 25  100e RNN_10.84% GRU 10.44% LSTM 10.38%  \n",
    "len 50  100e RNN 10.75% GRU 10.50% LSTM 10.55%  \n",
    "len 75  100e RNN 9.35% GRU 10.00% LSTM 10.26%  \n",
    "len 100 100e RNN 9.76% GRU 9.59% LSTM  9.62%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
